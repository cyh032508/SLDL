{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f85c7a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6842e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•¸æ“šé›†å½¢ç‹€ (åˆ—æ•¸, è¡Œæ•¸): (495387, 2)\n",
      "æ•¸æ“šé›†å‰ 5 è¡Œ:\n",
      "          name  price\n",
      "0   ! åå…«éº»æ²¹é´¨ç²½-ç†±     50\n",
      "1  ! å¤§ç¦ å°ç±³è‚‰ç²½-å‡    104\n",
      "2    ! ç­èŠ‹ç«¹é¦™ç²½-å‡    298\n",
      "3   ! ç¸½ è›‹é»ƒè‚‰ç²½-ç†±     42\n",
      "4   ! é®‘é­šå¹²è²è·é£¯-å‡    258\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import math\n",
    "\n",
    "file_name = 'train.csv' \n",
    "file_path = file_name \n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"æ•¸æ“šé›†å½¢ç‹€ (åˆ—æ•¸, è¡Œæ•¸): {df.shape}\")\n",
    "print(\"æ•¸æ“šé›†å‰ 5 è¡Œ:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316be0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- åˆå§‹åƒ¹æ ¼ç‰¹å¾µæå–çµæœ ---\n",
      "          name  initial_price\n",
      "0   ! åå…«éº»æ²¹é´¨ç²½-ç†±            0.0\n",
      "1  ! å¤§ç¦ å°ç±³è‚‰ç²½-å‡            0.0\n",
      "2    ! ç­èŠ‹ç«¹é¦™ç²½-å‡            0.0\n",
      "3   ! ç¸½ è›‹é»ƒè‚‰ç²½-ç†±            0.0\n",
      "4   ! é®‘é­šå¹²è²è·é£¯-å‡            0.0\n",
      "\n",
      "--- ç¶œåˆæ–‡æœ¬æ¸…æ´—çµæœ (ä¿ç•™è²¨å¹£ç¬¦è™Ÿ) ---\n",
      "          name cleaned_name\n",
      "0   ! åå…«éº»æ²¹é´¨ç²½-ç†±     åå…«éº»æ²¹é´¨ç²½ ç†±\n",
      "1  ! å¤§ç¦ å°ç±³è‚‰ç²½-å‡    å¤§ç¦ å°ç±³è‚‰ç²½ å‡\n",
      "2    ! ç­èŠ‹ç«¹é¦™ç²½-å‡      ç­èŠ‹ç«¹é¦™ç²½ å‡\n",
      "3   ! ç¸½ è›‹é»ƒè‚‰ç²½-ç†±     ç¸½ è›‹é»ƒè‚‰ç²½ ç†±\n",
      "4   ! é®‘é­šå¹²è²è·é£¯-å‡     é®‘é­šå¹²è²è·é£¯ å‡\n",
      "\n",
      "ç›®æ¨™è®Šé‡ Log è½‰æ›å®Œæˆï¼Œæ–°æ¬„ä½ç‚º 'log_price'\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 0.1 å¾å•†å“åç¨±æå–åˆå§‹åƒ¹æ ¼ç‰¹å¾µ\n",
    "# ----------------------------------------\n",
    "price_pattern = re.compile(r'(?i)(?:\\$|nt\\$?)\\s*(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "def extract_initial_price(name):\n",
    "    \"\"\"å¾å•†å“åç¨±æå–ä»¥ $ æˆ– NT é–‹é ­çš„åƒ¹æ ¼æ•¸å€¼ã€‚è‹¥æœªæ‰¾åˆ°åƒ¹æ ¼ï¼Œå›å‚³ 0ã€‚\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return 0.0\n",
    "    match = price_pattern.search(str(name))\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    return 0.0\n",
    "\n",
    "df['initial_price'] = df['name'].apply(extract_initial_price).fillna(0.0)\n",
    "print(\"--- åˆå§‹åƒ¹æ ¼ç‰¹å¾µæå–çµæœ ---\")\n",
    "print(df[['name', 'initial_price']].head())\n",
    "\n",
    "# ----------------------------------------\n",
    "# 0.2 ç¶œåˆæ–‡æœ¬æ¸…æ´—å‡½æ•¸ (ä¿ç•™è²¨å¹£ç¬¦è™Ÿ)\n",
    "# ----------------------------------------\n",
    "def comprehensive_clean_name_v3(name):\n",
    "    \"\"\"ä¿ç•™è²¨å¹£ç¬¦è™Ÿã€æ•¸å­—ã€å­—æ¯ã€ä¸­æ–‡ä¸¦ç§»é™¤å…¶ä»–ç¬¦è™Ÿã€‚\"\"\"\n",
    "    name = str(name).strip().lower()\n",
    "    name = name.replace(\"&quot;\", \" \")\n",
    "    name = re.sub(r'[^$\\u4e00-\\u9fa5a-z0-9\\s]', ' ', name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "df['cleaned_name'] = df['name'].apply(comprehensive_clean_name_v3)\n",
    "print(\"\\n--- ç¶œåˆæ–‡æœ¬æ¸…æ´—çµæœ (ä¿ç•™è²¨å¹£ç¬¦è™Ÿ) ---\")\n",
    "print(df[['name', 'cleaned_name']].head())\n",
    "\n",
    "# ----------------------------------------\n",
    "# 0.3 ç›®æ¨™è®Šé‡è½‰æ› (Price -> Log Price)\n",
    "# ----------------------------------------\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "print(f\"\\nç›®æ¨™è®Šé‡ Log è½‰æ›å®Œæˆï¼Œæ–°æ¬„ä½ç‚º 'log_price'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ef4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Loading model cost 0.372 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ä¸­æ–‡åˆ†è©çµæœ ---\n",
      "  cleaned_name  name_tokens\n",
      "0     åå…«éº»æ²¹é´¨ç²½ ç†±  åå…« éº»æ²¹ é´¨ ç²½ ç†±\n",
      "1    å¤§ç¦ å°ç±³è‚‰ç²½ å‡  å¤§ç¦ å°ç±³ è‚‰ ç²½ å‡\n",
      "2      ç­èŠ‹ç«¹é¦™ç²½ å‡    ç­èŠ‹ ç«¹é¦™ ç²½ å‡\n",
      "3     ç¸½ è›‹é»ƒè‚‰ç²½ ç†±    ç¸½ è›‹é»ƒè‚‰ ç²½ ç†±\n",
      "4     é®‘é­šå¹²è²è·é£¯ å‡   é®‘é­š å¹²è² è·é£¯ å‡\n",
      "æ•¸æ“šé›†åˆ†å‰²å®Œæˆ: è¨“ç·´é›†å¤§å°=396309, æ¸¬è©¦é›†å¤§å°=99078\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. ä¸­æ–‡åˆ†è© (Tokenization)\n",
    "# ----------------------------------------\n",
    "# å»ºè­°æ‚¨å®‰è£ jieba: !pip install jieba\n",
    "\n",
    "stopwords = set()  # å¯åœ¨æ­¤è¼‰å…¥é ˜åŸŸåœç”¨è©\n",
    "\n",
    "def segment_chinese_text(text):\n",
    "    \"\"\"ä½¿ç”¨ Jieba é€²è¡Œç²¾ç¢ºæ¨¡å¼åˆ†è©ï¼Œä¸¦ç§»é™¤åœç”¨è©ã€‚\"\"\"\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    tokens = [word for word in words if word.strip() != \"\" and word not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['name_tokens'] = df['cleaned_name'].apply(segment_chinese_text)\n",
    "print(\"--- ä¸­æ–‡åˆ†è©çµæœ ---\")\n",
    "print(df[['cleaned_name', 'name_tokens']].head())\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. æº–å‚™è¨“ç·´æ•¸æ“š (åŒæ™‚åˆ†å‰²æ–‡æœ¬èˆ‡åˆå§‹åƒ¹æ ¼ç‰¹å¾µ)\n",
    "# ----------------------------------------\n",
    "X_train_text, X_test_text, X_train_initial_price, X_test_initial_price, y_train, y_test = train_test_split(\n",
    "    df['name_tokens'],\n",
    "    df['initial_price'],\n",
    "    df['log_price'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"æ•¸æ“šé›†åˆ†å‰²å®Œæˆ: è¨“ç·´é›†å¤§å°={X_train_text.shape[0]}, æ¸¬è©¦é›†å¤§å°={X_test_text.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f1b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF å‘é‡åŒ–çµæœ ---\n",
      "ç¸½è©å½™è¡¨å¤§å° (ç‰¹å¾µç¶­åº¦): 50000\n",
      "è¨“ç·´é›†ç‰¹å¾µçŸ©é™£å½¢ç‹€: (396309, 50000)\n",
      "æ¸¬è©¦é›†ç‰¹å¾µçŸ©é™£å½¢ç‹€: (99078, 50000)\n",
      "--- ç‰¹å¾µæ‹¼æ¥çµæœ ---\n",
      "è¨“ç·´é›†æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: (396309, 50001)\n",
      "æ¸¬è©¦é›†æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: (99078, 50001)\n",
      "--- æ¨¡å‹è¨“ç·´ (Ridge) ---\n",
      "Ridge æ¨¡å‹è¨“ç·´å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 3. TF-IDF å‘é‡åŒ–\n",
    "# ----------------------------------------\n",
    "\n",
    "# æŒ‰æŒ‡ç¤ºä½¿ç”¨æŒ‡å®šåƒæ•¸é€²è¡Œå‘é‡åŒ–\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# åœ¨è¨“ç·´é›†ä¸Š fit (å­¸ç¿’è©å½™è¡¨å’Œæ¬Šé‡)\n",
    "X_train_vec = tfidf.fit_transform(X_train_text)\n",
    "\n",
    "# åœ¨æ¸¬è©¦é›†ä¸Š transform (ä½¿ç”¨è¨“ç·´é›†çš„è©å½™è¡¨)\n",
    "X_test_vec = tfidf.transform(X_test_text)\n",
    "\n",
    "print(f\"\\n--- TF-IDF å‘é‡åŒ–çµæœ ---\")\n",
    "print(f\"ç¸½è©å½™è¡¨å¤§å° (ç‰¹å¾µç¶­åº¦): {X_train_vec.shape[1]}\")\n",
    "print(f\"è¨“ç·´é›†ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_train_vec.shape}\")\n",
    "print(f\"æ¸¬è©¦é›†ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_test_vec.shape}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3.1 æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–èˆ‡ç‰¹å¾µæ‹¼æ¥\n",
    "# ----------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_price_scaled = scaler.fit_transform(X_train_initial_price.to_numpy().reshape(-1, 1))\n",
    "X_test_price_scaled = scaler.transform(X_test_initial_price.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# å°‡æ¨™æº–åŒ–å¾Œçš„æ•¸å€¼ç‰¹å¾µè½‰ç‚ºç¨€ç–çŸ©é™£ä¸¦èˆ‡ TF-IDF ç‰¹å¾µæ‹¼æ¥\n",
    "X_train_final = hstack([X_train_vec, csr_matrix(X_train_price_scaled)])\n",
    "X_test_final = hstack([X_test_vec, csr_matrix(X_test_price_scaled)])\n",
    "\n",
    "print(f\"--- ç‰¹å¾µæ‹¼æ¥çµæœ ---\")\n",
    "print(f\"è¨“ç·´é›†æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_train_final.shape}\")\n",
    "print(f\"æ¸¬è©¦é›†æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_test_final.shape}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. æ¨¡å‹è¨“ç·´ (Ridge Regression)\n",
    "# ----------------------------------------\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "print(\"--- æ¨¡å‹è¨“ç·´ (Ridge) ---\")\n",
    "# ä½¿ç”¨çµåˆå¾Œçš„ç‰¹å¾µé€²è¡Œè¨“ç·´\n",
    "ridge_model.fit(X_train_final, y_train)\n",
    "print(\"Ridge æ¨¡å‹è¨“ç·´å®Œæˆã€‚\")\n",
    "\n",
    "# ä¿ç•™æ¨¡å‹å¯¦ä¾‹ä¾›å¾ŒçºŒä½¿ç”¨\n",
    "model = ridge_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e04ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ¨¡å‹è©•ä¼° ---\n",
      "å¹³å‡çµ•å°èª¤å·® (MAE) - åŸå§‹åƒ¹æ ¼: 709.30\n",
      "å‡æ–¹æ ¹èª¤å·® (RMSE) - åŸå§‹åƒ¹æ ¼: 5342.11\n",
      "éƒ¨åˆ†é æ¸¬çµæœ:\n",
      "        Actual_Price  Predicted_Price\n",
      "412125         230.0       235.760484\n",
      "207881       41310.0     34267.467955\n",
      "353876        1199.0       737.647625\n",
      "210982         890.0       620.221040\n",
      "300354         495.0       807.706882\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 5. æ¨¡å‹è©•ä¼°\n",
    "# ----------------------------------------\n",
    "\n",
    "# ä½¿ç”¨æœ€çµ‚ç‰¹å¾µçŸ©é™£é€²è¡Œé æ¸¬\n",
    "y_pred_log = model.predict(X_test_final)\n",
    "\n",
    "# è½‰æ›å›åŸå§‹åƒ¹æ ¼ (Price)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# è¨ˆç®—è©•ä¼°æŒ‡æ¨™\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print(f\"\\n--- æ¨¡å‹è©•ä¼° ---\")\n",
    "print(f\"å¹³å‡çµ•å°èª¤å·® (MAE) - åŸå§‹åƒ¹æ ¼: {mae:.2f}\")\n",
    "print(f\"å‡æ–¹æ ¹èª¤å·® (RMSE) - åŸå§‹åƒ¹æ ¼: {rmse:.2f}\")\n",
    "\n",
    "# ç¤ºç¯„é æ¸¬çµæœ\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual_Price': y_true,\n",
    "    'Predicted_Price': y_pred\n",
    "})\n",
    "print(\"éƒ¨åˆ†é æ¸¬çµæœ:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae6e9d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82ee54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸè®€å–æ¸¬è©¦é›†: test.csv\n",
      "æ¸¬è©¦é›†å½¢ç‹€: (55043, 1)\n"
     ]
    }
   ],
   "source": [
    "# åŒ¯å…¥å¿…è¦çš„åº«\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. è®€å–æ¸¬è©¦é›†\n",
    "# ----------------------------------------------------\n",
    "test_file_path = 'test.csv'\n",
    "try:\n",
    "    df_test = pd.read_csv(test_file_path)\n",
    "    print(f\"æˆåŠŸè®€å–æ¸¬è©¦é›†: {test_file_path}\")\n",
    "    print(f\"æ¸¬è©¦é›†å½¢ç‹€: {df_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ '{test_file_path}'ã€‚æ­£åœ¨å‰µå»ºæ¼”ç¤ºæ•¸æ“šã€‚\")\n",
    "    df_test = pd.DataFrame({\n",
    "        'name': [\n",
    "            '! äºæ´²ç‰¹ç´šé¦™ç±³-ç†±',\n",
    "            '\"å¥‡å¦®\"æœ€æ–°æ¬¾é†«ç™‚ç”¨æŸå¸¶(æœªæ»…èŒ) GH20',\n",
    "            'NT$1590å“ç‰Œå¥³é‹å‹•é‹-2025ç‰ˆ',\n",
    "            '$99 ç©å…·ç©æœ¨'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. å®šç¾©æ¸…æ´—èˆ‡ç‰¹å¾µå·¥ç¨‹å‡½æ•¸ (éœ€èˆ‡è¨“ç·´éšæ®µä¸€è‡´)\n",
    "# ----------------------------------------\n",
    "price_pattern = re.compile(r'(?i)(?:\\$|nt\\$?)\\s*(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "def extract_initial_price(name):\n",
    "    if pd.isna(name):\n",
    "        return 0.0\n",
    "    match = price_pattern.search(str(name))\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group(1))\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    return 0.0\n",
    "\n",
    "def comprehensive_clean_name_v3(name):\n",
    "    name = str(name).strip().lower()\n",
    "    name = name.replace(\"&quot;\", \" \")\n",
    "    name = re.sub(r'[^$\\u4e00-\\u9fa5a-z0-9\\s]', ' ', name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "def segment_chinese_text(text):\n",
    "    words = jieba.cut(text, cut_all=False)\n",
    "    tokens = [word for word in words if word.strip() != \"\" and word not in stopwords]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "813e0e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ¸¬è©¦é›†æ¸…æ´—èˆ‡ç‰¹å¾µå·¥ç¨‹çµæœ (å‰ 5 ç­†) ---\n",
      "                          name  initial_price                   name_tokens\n",
      "0                    !(å‘·)è›‹é»ƒç²½-ç†±            0.0                      å‘· è›‹é»ƒ ç²½ ç†±\n",
      "1  #13-PILOTå¯æ“¦å°ç«  FRIXION stamp            0.0  13 pilot å¯æ“¦ å°ç«  frixion stamp\n",
      "2  #17-PILOTå¯æ“¦å°ç«  FRIXION stamp            0.0  17 pilot å¯æ“¦ å°ç«  frixion stamp\n",
      "3                #304éš”ç†±æ¹¯ç¢—çµ„16CM            0.0              304 éš”ç†± æ¹¯ç¢— çµ„ 16cm\n",
      "4               #316æ–¹å½¢ç­·ç›’çµ„-20cm            0.0              316 æ–¹å½¢ ç­·ç›’ çµ„ 20cm\n",
      "\n",
      "--- æ¸¬è©¦é›†ç‰¹å¾µçŸ©é™£ ---\n",
      "TF-IDF ç‰¹å¾µçŸ©é™£å½¢ç‹€: (55043, 50000)\n",
      "æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: (55043, 50001)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. åœ¨æ¸¬è©¦é›†ä¸Šå¥—ç”¨æ¸…æ´—ã€åˆ†è©èˆ‡åˆå§‹åƒ¹æ ¼æå–\n",
    "# ----------------------------------------\n",
    "df_test['initial_price'] = df_test['name'].apply(extract_initial_price).fillna(0.0)\n",
    "df_test['cleaned_name'] = df_test['name'].apply(comprehensive_clean_name_v3)\n",
    "df_test['name_tokens'] = df_test['cleaned_name'].apply(segment_chinese_text)\n",
    "\n",
    "print(\"\\n--- æ¸¬è©¦é›†æ¸…æ´—èˆ‡ç‰¹å¾µå·¥ç¨‹çµæœ (å‰ 5 ç­†) ---\")\n",
    "print(df_test[['name', 'initial_price', 'name_tokens']].head())\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. æ‡‰ç”¨ TF-IDF è½‰æ›èˆ‡æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–\n",
    "# ----------------------------------------\n",
    "if 'tfidf' not in locals():\n",
    "    raise NameError(\"éŒ¯èª¤: æ‰¾ä¸åˆ° 'tfidf' è®Šæ•¸ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´é›†çš„ç¨‹å¼ç¢¼ã€‚\")\n",
    "if 'scaler' not in locals():\n",
    "    raise NameError(\"éŒ¯èª¤: æ‰¾ä¸åˆ° 'scaler' è®Šæ•¸ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´é›†çš„ç¨‹å¼ç¢¼ã€‚\")\n",
    "\n",
    "X_test_vec_final = tfidf.transform(df_test['name_tokens'])\n",
    "initial_price_scaled = scaler.transform(df_test[\"initial_price\"].to_numpy().reshape(-1, 1))\n",
    "X_test_final = hstack([X_test_vec_final, csr_matrix(initial_price_scaled)])\n",
    "\n",
    "print(f\"\\n--- æ¸¬è©¦é›†ç‰¹å¾µçŸ©é™£ ---\")\n",
    "print(f\"TF-IDF ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_test_vec_final.shape}\")\n",
    "print(f\"æœ€çµ‚ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_test_final.shape}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. æ¨¡å‹é æ¸¬ (ä½¿ç”¨å·²è¨“ç·´å¥½çš„ Ridge æ¨¡å‹)\n",
    "# ----------------------------------------\n",
    "if 'model' not in locals():\n",
    "    raise NameError(\"éŒ¯èª¤: æ‰¾ä¸åˆ° 'model' è®Šæ•¸ã€‚è«‹å…ˆé‹è¡Œè¨“ç·´é›†çš„ç¨‹å¼ç¢¼ã€‚\")\n",
    "\n",
    "# é æ¸¬ log_price ä¸¦é‚„åŸè‡³åŸå§‹åƒ¹æ ¼\n",
    "y_pred_log_test = model.predict(X_test_final)\n",
    "y_pred_price_test = np.expm1(y_pred_log_test)\n",
    "df_test['price'] = y_pred_price_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa559e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æœ€çµ‚é æ¸¬çµæœ (å‰ 5 ç­†) ---\n",
      "                          name       price\n",
      "0                    !(å‘·)è›‹é»ƒç²½-ç†±  174.916932\n",
      "1  #13-PILOTå¯æ“¦å°ç«  FRIXION stamp  102.425269\n",
      "2  #17-PILOTå¯æ“¦å°ç«  FRIXION stamp   91.428596\n",
      "3                #304éš”ç†±æ¹¯ç¢—çµ„16CM  166.532301\n",
      "4               #316æ–¹å½¢ç­·ç›’çµ„-20cm  173.735604\n",
      "\n",
      "é æ¸¬çµæœå·²å„²å­˜åˆ° 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 4. è¼¸å‡ºçµæœ\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"\\n--- æœ€çµ‚é æ¸¬çµæœ (å‰ 5 ç­†) ---\")\n",
    "print(df_test[['name', 'price']].head())\n",
    "\n",
    "# å„²å­˜çµæœåˆ° CSV æ–‡ä»¶\n",
    "output_filename = 'test_predictions.csv'\n",
    "df_test[['name', 'price']].to_csv(output_filename, index=False, float_format='%.2f')\n",
    "\n",
    "print(f\"\\né æ¸¬çµæœå·²å„²å­˜åˆ° '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f43e84",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e630e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RÂ²: 0.023200342530669937\n",
      "MAE: 1295.4936923276314\n",
      "RMSE: 711398059.5035032\n",
      "\n",
      "ğŸ’° æé«˜åƒ¹æ ¼çš„é—œéµè©ï¼š\n",
      "            feature          coef\n",
      "1758             å¥è¡Œ  69415.961909\n",
      "1363  timberland ç”·æ¬¾  68057.706298\n",
      "1362     timberland  52275.260413\n",
      "2517             å°éº¥  40187.663504\n",
      "843              i7  19090.434224\n",
      "1583             ä¸­ç­’  17847.292231\n",
      "3754             ç£¨ç ‚  17430.304845\n",
      "330            512g  16654.348128\n",
      "3617             ç”·æ¬¾  15609.939726\n",
      "4352             è®Šé »  15564.050020\n",
      "\n",
      "ğŸ’¸ é™ä½åƒ¹æ ¼çš„é—œéµè©ï¼š\n",
      "      feature         coef\n",
      "689     edwin -2488.680280\n",
      "1239  sandisk -2505.327958\n",
      "3424       æ¿¾ç¶² -2508.083641\n",
      "2195    å›ºæ…‹ ç¡¬ç¢Ÿ -2519.377740\n",
      "2037    åŸå»  å…¬å¸ -2920.643262\n",
      "3721       çŸ­è¢– -2935.592689\n",
      "3663       çš®å¥— -3029.479875\n",
      "1724       ä¿è­· -3198.117542\n",
      "1732      ä¿è­·æ®¼ -3479.190625\n",
      "3636       ç™»å±± -5041.630944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ”§ åœç”¨è©\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "chinese_stopwords = {\n",
    "    \"çš„\", \"äº†\", \"å’Œ\", \"æ˜¯\", \"æˆ‘\", \"ä½ \", \"ä»–\", \"å¥¹\", \"å®ƒ\",\n",
    "    \"æˆ‘å€‘\", \"ä½ å€‘\", \"ä»–å€‘\", \"å®ƒå€‘\", \"é€™å€‹\", \"é‚£å€‹\", \"å…¨æ–°\", \"ç¾è²¨\", \"å…é‹\",\n",
    "    \"å•†å“\", \"åŸè£\", \"ä¿è­‰\", \"è¶…å€¼\", \"æœ€æ–°\", \"ä¿ƒéŠ·\", \"é™é‡\", \"ç‰¹åƒ¹\", \"æ­£å“\", \"åŒ…éƒµ\"\n",
    "}\n",
    "stopwords_all = english_stopwords | chinese_stopwords\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ§¹ é è™•ç†å‡½å¼\n",
    "def preprocess_product_name(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([a-z0-9]+)\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\u4e00-\\u9fff ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = jieba.lcut(text)\n",
    "    words = [w for w in words if w.strip() and w not in stopwords_all]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ§  å»ºç«‹ Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        preprocessor=preprocess_product_name,\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    (\"model\", RidgeCV(alphas=np.logspace(-2, 2, 10), cv=5))\n",
    "])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ§® ç¤ºç¯„è³‡æ–™\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "# ------------------------------------------------\n",
    "# ğŸš€ è¨“ç·´æ¨¡å‹\n",
    "pipeline.fit(df[\"name\"], df[\"price\"])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ“Š é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = pipeline.predict(df[\"name\"])\n",
    "print(\"RÂ²:\", r2_score(df[\"price\"], y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(df[\"price\"], y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(df[\"price\"], y_pred))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ğŸ” è§€å¯Ÿ TF-IDF æ¬Šé‡æœ€é«˜çš„è©\n",
    "tfidf = pipeline.named_steps[\"tfidf\"]\n",
    "model = pipeline.named_steps[\"model\"]\n",
    "\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": model.coef_\n",
    "}).sort_values(by=\"coef\", ascending=False)\n",
    "\n",
    "print(\"\\nğŸ’° æé«˜åƒ¹æ ¼çš„é—œéµè©ï¼š\")\n",
    "print(coef_df.head(10))\n",
    "\n",
    "print(\"\\nğŸ’¸ é™ä½åƒ¹æ ¼çš„é—œéµè©ï¼š\")\n",
    "print(coef_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1aa625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          name        price\n",
      "0                    !(å‘·)è›‹é»ƒç²½-ç†±  1223.280674\n",
      "1  #13-PILOTå¯æ“¦å°ç«  FRIXION stamp  2593.867919\n",
      "2  #17-PILOTå¯æ“¦å°ç«  FRIXION stamp  3068.384807\n",
      "3                #304éš”ç†±æ¹¯ç¢—çµ„16CM   442.106147\n",
      "4               #316æ–¹å½¢ç­·ç›’çµ„-20cm   670.524270\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "y_test_pred = pipeline.predict(test_df[\"name\"])\n",
    "test_df[\"price\"] = y_test_pred\n",
    "print(test_df.head())\n",
    "test_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10275d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 845.26\n",
      "RMSE: 5608.26\n",
      "         Actual     Predicted\n",
      "412125    230.0    291.289140\n",
      "207881  41310.0  33411.087323\n",
      "353876   1199.0   1501.927664\n",
      "210982    890.0    542.505490\n",
      "300354    495.0    783.392727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer # ç”¨æ–¼è‡ªè¨‚è©•åˆ†å™¨\n",
    "import math\n",
    "\n",
    "# =============================================\n",
    "# ğŸ”§ 1ï¸âƒ£ è‡ªè¨‚å‰è™•ç† Transformer\n",
    "# =============================================\n",
    "\n",
    "price_pattern = re.compile(r'(?i)(?:\\$|nt\\$?)\\s*(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "class PriceExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"å¾å•†å“åç¨±æå–åˆå§‹åƒ¹æ ¼\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        def extract_initial_price(name):\n",
    "            if pd.isna(name):\n",
    "                return 0.0\n",
    "            match = price_pattern.search(str(name))\n",
    "            if match:\n",
    "                try:\n",
    "                    return float(match.group(1))\n",
    "                except ValueError:\n",
    "                    return 0.0\n",
    "            return 0.0\n",
    "        prices = X.apply(extract_initial_price).fillna(0.0).to_numpy().reshape(-1, 1)\n",
    "        return prices\n",
    "\n",
    "\n",
    "class NameCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"æ¸…æ´—èˆ‡åˆ†è©å•†å“åç¨±\"\"\"\n",
    "    def __init__(self, stopwords=None):\n",
    "        # â— ä¸è¦ç”¨å¯è®Šçš„ set() ç•¶é è¨­å€¼\n",
    "        # æ”¹æˆç”¨ None ä¸¦åœ¨ fit() åˆå§‹åŒ–\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.stopwords is None:\n",
    "            self.stopwords_ = frozenset()  # ä¸å¯è®Šé›†åˆ\n",
    "        else:\n",
    "            self.stopwords_ = frozenset(self.stopwords)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cleaned = []\n",
    "        for name in X:\n",
    "            name = str(name).strip().lower()\n",
    "            name = name.replace(\"&quot;\", \" \")\n",
    "            name = re.sub(r'[^$\\u4e00-\\u9fa5a-z0-9\\s]', ' ', name)\n",
    "            name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "            words = jieba.cut(name, cut_all=False)\n",
    "            tokens = [w for w in words if w.strip() and w not in self.stopwords_]\n",
    "            cleaned.append(\" \".join(tokens))\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ 2ï¸âƒ£ Pipeline å®šç¾©\n",
    "# =============================================\n",
    "\n",
    "# æ–‡å­—ç‰¹å¾µç®¡ç·šï¼šæ¸…æ´—ï¼‹TF-IDF\n",
    "text_pipeline = Pipeline([\n",
    "    (\"cleaner\", NameCleaner()),\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1, 2)))\n",
    "])\n",
    "\n",
    "# æ•¸å€¼ç‰¹å¾µç®¡ç·šï¼šæå–åˆå§‹åƒ¹æ ¼ï¼‹æ¨™æº–åŒ–\n",
    "price_pipeline = Pipeline([\n",
    "    (\"extract\", PriceExtractor()),\n",
    "    (\"scale\", StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# çµ„åˆç‰¹å¾µ\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"text\", text_pipeline, \"name\"),\n",
    "    (\"price\", price_pipeline, \"name\")\n",
    "])\n",
    "\n",
    "# ä¸» Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# ğŸ§ª 3ï¸âƒ£ è³‡æ–™è¼‰å…¥èˆ‡è¨“ç·´\n",
    "# =============================================\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# log åƒ¹æ ¼ä½œç‚ºç›®æ¨™\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"log_price\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# è¨“ç·´\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬\n",
    "y_pred_log = model_pipeline.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“Š 4ï¸âƒ£ æ¨¡å‹è©•ä¼°\n",
    "# =============================================\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_true,\n",
    "    \"Predicted\": y_pred\n",
    "})\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20f00bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- è¨“ç·´èˆ‡èª¿å„ªé–‹å§‹ï¼šRidge + GridSearchCV (å„ªåŒ– SMAPE) ---\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Loading model cost 0.576 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.586 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.680 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/jieba.cache\n",
      "Loading model cost 0.681 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.592 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.555 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.545 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................................model__alpha=1.0; total time=  40.3s\n",
      "[CV] END ...................................model__alpha=1.0; total time=  40.0s\n",
      "[CV] END ...................................model__alpha=1.0; total time=  40.6s\n",
      "[CV] END ...................................model__alpha=0.1; total time=  41.0s\n",
      "[CV] END ...................................model__alpha=0.1; total time=  41.3s\n",
      "[CV] END ...................................model__alpha=0.1; total time=  41.9s\n",
      "[CV] END ...................................model__alpha=0.1; total time=  42.0s\n",
      "[CV] END ...................................model__alpha=0.1; total time=  42.4s\n",
      "[CV] END ...................................model__alpha=1.0; total time=  39.3s\n",
      "[CV] END ..................................model__alpha=10.0; total time=  39.6s\n",
      "[CV] END ..................................model__alpha=10.0; total time=  39.9s\n",
      "[CV] END ..................................model__alpha=10.0; total time=  39.8s\n",
      "[CV] END ...................................model__alpha=1.0; total time=  40.7s\n",
      "[CV] END ..................................model__alpha=10.0; total time=  39.6s\n",
      "[CV] END ..................................model__alpha=50.0; total time=  39.5s\n",
      "[CV] END ..................................model__alpha=10.0; total time=  40.0s\n",
      "[CV] END ..................................model__alpha=50.0; total time=  37.2s\n",
      "[CV] END ..................................model__alpha=50.0; total time=  37.3s\n",
      "[CV] END ..................................model__alpha=50.0; total time=  38.2s\n",
      "[CV] END ..................................model__alpha=50.0; total time=  37.9s\n",
      "[CV] END .................................model__alpha=100.0; total time=  38.2s\n",
      "[CV] END .................................model__alpha=100.0; total time=  38.3s\n",
      "[CV] END .................................model__alpha=100.0; total time=  38.6s\n",
      "[CV] END .................................model__alpha=100.0; total time=  38.7s\n",
      "[CV] END .................................model__alpha=100.0; total time=  32.4s\n",
      "[CV] END .................................model__alpha=500.0; total time=  32.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/knrkn_dn3l38qxkbzxpx06g40000gn/T/ipykernel_72849/2175959613.py:27: RuntimeWarning: overflow encountered in expm1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................model__alpha=500.0; total time=  31.8s\n",
      "[CV] END .................................model__alpha=500.0; total time=  32.4s\n",
      "[CV] END .................................model__alpha=500.0; total time=  32.2s\n",
      "[CV] END .................................model__alpha=500.0; total time=  32.4s\n",
      "\n",
      "--- è¨“ç·´çµæœ ---\n",
      "æœ€ä½³ Ridge alpha: 1.0\n",
      "æœ€ä½³ 5-Fold CV SMAPE: 47.9974%\n",
      "\n",
      "--- æ¸¬è©¦é›†æœ€çµ‚è©•ä¼° ---\n",
      "Symmetric Mean Absolute Percentage Error (SMAPE): 47.7161%\n",
      "å¹³å‡çµ•å°èª¤å·® (MAE): 708.82\n",
      "å‡æ–¹æ ¹èª¤å·® (RMSE): 5339.09\n",
      "\n",
      "éƒ¨åˆ†é æ¸¬çµæœ (å‰ 5 ç­†):\n",
      "         Actual  Predicted\n",
      "412125    230.0     238.84\n",
      "207881  41310.0   35439.79\n",
      "353876   1199.0     718.67\n",
      "210982    890.0     608.49\n",
      "300354    495.0     798.33\n",
      "\n",
      "--- æ­£åœ¨ç‚ºæ¸¬è©¦é›†ç”Ÿæˆæœ€çµ‚é æ¸¬çµæœ ---\n",
      "\n",
      "âœ… æäº¤æª”æ¡ˆå·²æˆåŠŸå„²å­˜è‡³: 'kaggle_submission_ridge_optimized.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "# =============================================\n",
    "# ğŸ¯ 0ï¸âƒ£ ç«¶è³½è©•ä¼°æ¨™æº–ï¼šSMAPE å‡½æ•¸\n",
    "# =============================================\n",
    "\n",
    "def smape(y_true_log, y_pred_log):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    ç”¨æ–¼ GridSearchCV è©•ä¼°å’Œæœ€çµ‚çµæœå ±å‘Šã€‚\n",
    "    \"\"\"\n",
    "    # 1. å°‡ log åƒ¹æ ¼é‚„åŸç‚ºåŸå§‹åƒ¹æ ¼\n",
    "    y_true_orig = np.expm1(y_true_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    \n",
    "    # 2. é¿å…é æ¸¬è² å€¼\n",
    "    y_pred_orig[y_pred_orig < 0] = 0\n",
    "    \n",
    "    # 3. è¨ˆç®— SMAPE\n",
    "    numerator = np.abs(y_pred_orig - y_true_orig)\n",
    "    denominator = (np.abs(y_true_orig) + np.abs(y_pred_orig)) / 2\n",
    "    \n",
    "    # é¿å…é™¤ä»¥é›¶\n",
    "    non_zero_mask = denominator != 0\n",
    "    smape_val = np.mean(numerator[non_zero_mask] / denominator[non_zero_mask]) * 100\n",
    "    \n",
    "    return smape_val\n",
    "\n",
    "# å°‡ SMAPE è½‰æ›ç‚º Sklearn è©•åˆ†å™¨ï¼ˆGridSearchCV é è¨­æœ€å¤§åŒ–åˆ†æ•¸ï¼Œå› æ­¤å–è² å€¼ï¼‰\n",
    "smap_scorer = make_scorer(smape, greater_is_better=False)\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ğŸ”§ 1ï¸âƒ£ è‡ªè¨‚å‰è™•ç† Transformer\n",
    "# =============================================\n",
    "\n",
    "price_pattern = re.compile(r'(?i)(?:\\$|nt\\$?)\\s*(\\d+(?:\\.\\d+)?)')\n",
    "\n",
    "class PriceExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"å¾å•†å“åç¨±æå–åˆå§‹åƒ¹æ ¼\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        def extract_initial_price(name):\n",
    "            if pd.isna(name):\n",
    "                return 0.0\n",
    "            match = price_pattern.search(str(name))\n",
    "            if match:\n",
    "                try:\n",
    "                    return float(match.group(1))\n",
    "                except ValueError:\n",
    "                    return 0.0\n",
    "            return 0.0\n",
    "        # è¼¸å‡ºç‚º NumPy é™£åˆ—ï¼Œç¬¦åˆ Scikit-learn æ…£ä¾‹\n",
    "        prices = X.apply(extract_initial_price).fillna(0.0).to_numpy().reshape(-1, 1)\n",
    "        return prices\n",
    "\n",
    "\n",
    "class NameCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"æ¸…æ´—èˆ‡åˆ†è©å•†å“åç¨±ï¼Œä¸¦ç§»é™¤å¢å¼·åœç”¨è©\"\"\"\n",
    "    def __init__(self, stopwords=None):\n",
    "        # ä¿®æ­£: åƒ…è¨­ç½®å¯¦ä¾‹å±¬æ€§ï¼Œä¸åšä»»ä½•åƒæ•¸ä¿®æ”¹\n",
    "        # å¦‚æœ stopwords æ˜¯å¯è®Šçš„ï¼Œå‰‡è¤‡è£½ä¸€ä»½\n",
    "        self.stopwords = list(stopwords) if stopwords is not None else None\n",
    "        \n",
    "        # é€™æ˜¯å›ºå®šçš„é è¨­åœç”¨è©ï¼Œå¯ä»¥åœ¨ fit ä¸­èˆ‡ self.stopwords çµåˆ\n",
    "        self.default_stopwords = {\n",
    "            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å€‹', 'çµ„', 'ä»¶', 'å…¥', \n",
    "            'çµ„', 'é¡†', 'æ¬¾', 'å‹', 'ä»£', 'è™Ÿ', 'æ–°', 'èˆŠ', 'ç‰ˆ', 'å…é‹', \n",
    "            'ç‰¹åƒ¹', 'ä¿ƒéŠ·', 'ç†±éŠ·', 'è²·', 'é€', 'åŠ è³¼', 'é™å®š', 'è¶…å€¼',\n",
    "            'è¶…å•†', 'ç¾è²¨', 'é è³¼', 'å…¨æ–°', 'ç‰¹æƒ ', 'å¤§è™Ÿ', 'å°è™Ÿ', 'å¥—è£'\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # ä¿®æ­£: åœ¨ fit éšæ®µåŸ·è¡Œçµåˆå’Œè½‰æ›ç‚º frozenset\n",
    "        final_stopwords = self.default_stopwords.copy()\n",
    "        if self.stopwords is not None:\n",
    "            final_stopwords.update(self.stopwords)\n",
    "            \n",
    "        self.stopwords_ = frozenset(final_stopwords)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        cleaned = []\n",
    "        for name in X:\n",
    "            name = str(name).strip().lower()\n",
    "            name = name.replace(\"&quot;\", \" \")\n",
    "            # ä¿ç•™è²¨å¹£ç¬¦è™Ÿã€ä¸­æ–‡å­—ã€å­—æ¯ã€æ•¸å­—å’Œç©ºæ ¼\n",
    "            name = re.sub(r'[^$\\u4e00-\\u9fa5a-z0-9\\s]', ' ', name)\n",
    "            name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "            \n",
    "            # ä½¿ç”¨ Jieba åˆ†è©\n",
    "            words = jieba.cut(name, cut_all=False)\n",
    "            tokens = [w for w in words if w.strip() and w not in self.stopwords_]\n",
    "            cleaned.append(\" \".join(tokens))\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ 2ï¸âƒ£ Pipeline å®šç¾©\n",
    "# =============================================\n",
    "\n",
    "# æ–‡å­—ç‰¹å¾µç®¡ç·šï¼šæ¸…æ´—ï¼‹TF-IDF (åŒ…å« Bigram N-grams)\n",
    "text_pipeline = Pipeline([\n",
    "    (\"cleaner\", NameCleaner()),\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1, 2)))\n",
    "])\n",
    "\n",
    "# æ•¸å€¼ç‰¹å¾µç®¡ç·šï¼šæå–åˆå§‹åƒ¹æ ¼ï¼‹æ¨™æº–åŒ–\n",
    "price_pipeline = Pipeline([\n",
    "    (\"extract\", PriceExtractor()),\n",
    "    (\"scale\", StandardScaler()) # ç§»é™¤ with_mean=Falseï¼Œä½¿ç”¨æ¨™æº–çš„ StandardScaler\n",
    "])\n",
    "\n",
    "# çµ„åˆç‰¹å¾µï¼šColumnTransformer è² è²¬å°‡ä¸åŒæ¬„ä½å‚³çµ¦ä¸åŒçš„ Pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    # 'name' æ¬„ä½åŒæ™‚ç”¨æ–¼æ–‡æœ¬è™•ç†å’Œåƒ¹æ ¼æå–\n",
    "    (\"text\", text_pipeline, \"name\"), \n",
    "    (\"price\", price_pipeline, \"name\")\n",
    "])\n",
    "\n",
    "# ä¸» Pipelineï¼šå‰è™•ç†ï¼‹æ¨¡å‹\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    # é€™è£¡çš„ Ridge æ¨¡å‹åªæ˜¯ä¸€å€‹ä½”ä½ç¬¦ï¼Œæœ€çµ‚æœƒè¢« GridSearchCV èª¿æ•´\n",
    "    (\"model\", Ridge(random_state=42)) \n",
    "])\n",
    "\n",
    "# =============================================\n",
    "# ğŸ§ª 3ï¸âƒ£ è³‡æ–™è¼‰å…¥èˆ‡è¨“ç·´ (ä½¿ç”¨ GridSearchCV é€²è¡Œèª¿å„ª)\n",
    "# =============================================\n",
    "\n",
    "# å‡è¨­ 'train.csv' åœ¨ç•¶å‰å·¥ä½œç›®éŒ„\n",
    "try:\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"éŒ¯èª¤: æ‰¾ä¸åˆ° 'train.csv'ï¼Œè«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘ã€‚\")\n",
    "    exit()\n",
    "\n",
    "# log åƒ¹æ ¼ä½œç‚ºç›®æ¨™\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "\n",
    "# æ•¸æ“šåˆ†å‰²ï¼šä½¿ç”¨ 80% è¨“ç·´é›†ï¼Œ20% æ¸¬è©¦é›†é€²è¡Œæœ¬åœ°è©•ä¼°\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"log_price\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# --- åƒæ•¸èª¿å„ªè¨­å®š ---\n",
    "param_grid = {\n",
    "    # 'model' æ˜¯ Pipeline ä¸­ Ridge æ¨¡å‹çš„åç¨±\n",
    "    'model__alpha': [0.1, 1.0, 10.0, 50.0, 100.0, 500.0] \n",
    "}\n",
    "\n",
    "# å®šç¾© K-Fold äº¤å‰é©—è­‰ç­–ç•¥\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# å‰µå»º GridSearchCVï¼Œä½¿ç”¨ SMAPE ä½œç‚ºè©•åˆ†æ¨™æº–\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_pipeline, \n",
    "    param_grid=param_grid,\n",
    "    scoring=smap_scorer, # ä½¿ç”¨ SMAPE è©•åˆ†å™¨\n",
    "    cv=cv_strategy,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"--- è¨“ç·´èˆ‡èª¿å„ªé–‹å§‹ï¼šRidge + GridSearchCV (å„ªåŒ– SMAPE) ---\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ç²å–æœ€ä½³æ¨¡å‹\n",
    "best_model = grid_search.best_estimator_\n",
    "best_alpha = best_model.named_steps['model'].alpha\n",
    "best_cv_smape = -grid_search.best_score_\n",
    "\n",
    "print(\"\\n--- è¨“ç·´çµæœ ---\")\n",
    "print(f\"æœ€ä½³ Ridge alpha: {best_alpha}\")\n",
    "print(f\"æœ€ä½³ 5-Fold CV SMAPE: {best_cv_smape:.4f}%\")\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“Š 4ï¸âƒ£ æ¨¡å‹è©•ä¼°èˆ‡æœ€çµ‚å ±å‘Š\n",
    "# =============================================\n",
    "\n",
    "# ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "\n",
    "# è½‰æ›å›åŸå§‹åƒ¹æ ¼\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# è¨ˆç®—æœ€çµ‚è©•ä¼°æŒ‡æ¨™\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "final_smape = smape(y_test, y_pred_log) \n",
    "\n",
    "print(\"\\n--- æ¸¬è©¦é›†æœ€çµ‚è©•ä¼° ---\")\n",
    "print(f\"Symmetric Mean Absolute Percentage Error (SMAPE): {final_smape:.4f}%\")\n",
    "print(f\"å¹³å‡çµ•å°èª¤å·® (MAE): {mae:.2f}\")\n",
    "print(f\"å‡æ–¹æ ¹èª¤å·® (RMSE): {rmse:.2f}\")\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Actual\": y_true,\n",
    "    \"Predicted\": y_pred\n",
    "})\n",
    "print(\"\\néƒ¨åˆ†é æ¸¬çµæœ (å‰ 5 ç­†):\")\n",
    "print(results.head().round(2))\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“¤ 5ï¸âƒ£ æ¸¬è©¦é›†é æ¸¬èˆ‡æª”æ¡ˆè¼¸å‡º (ç‚º Kaggle æäº¤æº–å‚™)\n",
    "# =============================================\n",
    "\n",
    "try:\n",
    "    df_test = pd.read_csv(\"test.csv\")\n",
    "    print(\"\\n--- æ­£åœ¨ç‚ºæ¸¬è©¦é›†ç”Ÿæˆæœ€çµ‚é æ¸¬çµæœ ---\")\n",
    "    \n",
    "    # ä½¿ç”¨æœ€ä½³æ¨¡å‹å°å®Œæ•´çš„æ¸¬è©¦é›†é€²è¡Œé æ¸¬\n",
    "    test_pred_log = best_model.predict(df_test)\n",
    "    test_pred_price = np.expm1(test_pred_log)\n",
    "    \n",
    "    # ç¢ºä¿åƒ¹æ ¼ä¸ç‚ºè² å€¼\n",
    "    test_pred_price[test_pred_price < 0] = 0\n",
    "    \n",
    "    # å‰µå»ºæäº¤æ–‡ä»¶ (å‡è¨­æ¸¬è©¦é›†åŒ…å« 'item_id' æˆ–å…¶ä»–å”¯ä¸€è­˜åˆ¥æ¬„ä½)\n",
    "    # é€™è£¡å‡è¨­æ‚¨çš„æ¸¬è©¦é›†åªéœ€è¦ 'name' å’Œé æ¸¬çš„ 'price'\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        \"name\": df_test[\"name\"],\n",
    "        \"price\": test_pred_price.round(2)\n",
    "    })\n",
    "    \n",
    "    output_filename = 'kaggle_submission_ridge_optimized.csv'\n",
    "    submission_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… æäº¤æª”æ¡ˆå·²æˆåŠŸå„²å­˜è‡³: '{output_filename}'\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸ æ‰¾ä¸åˆ° 'test.csv'ï¼Œè·³éç”Ÿæˆæœ€çµ‚æäº¤æª”æ¡ˆæ­¥é©Ÿã€‚\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
